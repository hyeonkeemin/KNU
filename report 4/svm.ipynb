{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "166d3ed6-2053-40d5-8643-df8b79e5eeaf",
   "metadata": {},
   "source": [
    "# SVM(Support Vector Machine, 서포트 벡터 머신)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e3357e-77ec-4491-b422-c541f4c30d68",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. 마진과 서포트 벡터\n",
    "\n",
    "- 샘플 공간에서 분할 초평면은 다음 선형 방정식을 통해 묘사 될 수 있다.\n",
    "    - $$ w^tx + b=0 $$\n",
    "<br/>\n",
    "- 여기서 $w = (w_1 ; w_2 ; \\cdots ; w_d)$는 법선 벡터이고 초 평면의 방향을 결정한다.\n",
    "- $b$는 변위항으로 초평면과 원점 간의 거리를 결정한다. \n",
    "- 다시말해, 분할 초평면은 법선 벡터 $w$와 변위 $b$에 의해 결정되고, 아래에서 $(w, b)$로 나타내며, 샘플공간에서 임의점 $x$에서 평면 $(w, b)$까지의 거리는 아래처럼 나타낼 수 있다.\n",
    "    - $$r = \\frac{w^Tx + b}{\\parallel w\\parallel}$$\n",
    "<br/><br/>\n",
    "- 초평면 $(w, b)$가 훈련 샘플을 정확히 분류할 수 있다고 가정한다면 $(x_i, y_i) \\in D$에서 $y_i = +1$이면 $w^Tx_i + b>0$ 이고, $y_i = -1$이면 $w^Tx_i + b < 0$이다.\n",
    "    - $$\\begin{cases}w^Tx_i +b\\geq +1, & y_i = +1; \\\\w^Tx_i +b\\leq -1, & y_i=-1.\\end{cases}$$\n",
    "<br/><br/>\n",
    "- 초평면에 가장 가까운 몇 개의 훈련 샘플 포인트는 등호에 해당하고 이들을 `서포트 벡터(Support Vector)`라고 부른다. \n",
    "- 두개의 서로 다른 클래스의 서포트 벡터에서 초평면에 달하는 거리의 합을 `마진(margin)`이라고 한다.\n",
    "    - $$\\gamma = \\frac{2}{\\parallel w\\parallel}$$\n",
    "<br/><br/>\n",
    "- 최대 마진(maximum margin)을 가지는 분할 초평면을 가지고 싶다면 아래의 제약 조건을 만족하는 파라미터 $w$와 $b$를 찾아 $\\gamma$를 최대화 하여야 함.\n",
    "    - $$\\max_{w, b}  \\frac{2}{\\parallel w\\parallel} \\\\ s.t.~~y_i(w^Tx_i) + b \\geq1,~~i =1,2,\\cdots,m.$$\n",
    "<br/><br/>\n",
    "- 이것이 바로 서포트 벡터 머신의 기본 모델이다.\n",
    "\n",
    "<br/><br/><br/><br/><br/>\n",
    "## 2 Dual Problem(쌍대 문제)\n",
    "- 우리는 아래의 식을 구해서 최대 마진을 가지는 모델을 구하고자 한다.\n",
    "    - $$f(x) = w^Tx+b$$\n",
    "<br/><br/>\n",
    "- 위 식 자체가 하나의 convex quardratic programming 문제인 것을 알 수 있기 때문에 전역해를 보장하고, 최적화 계산법을 동원하여 해를 구할 수 있다. 하지만 여기에 더 효과적인 방법이 존재한다.\n",
    "- 위 식에 라그랑주 승수법을 쓴다면 우리는 `Dual problem`을 얻을 수 있다. 구체적으로, 각 제약 조건에 라그랑주 승수 $\\alpha \\geq0$을 추가하면 해당 문제의 라그랑주 함수는 아래처럼 쓸 수 있다.\n",
    "    - $$L(w,b,\\alpha) =\\frac{1}{2}\\parallel w\\parallel^2+\\sum_{i=1}^m \\alpha_i(1-y_i(w^Tx_i+b))~~~~~(1),$$\n",
    "<br/><br/>\n",
    "- 여기서 $\\alpha=(\\alpha_1;\\alpha_2;\\cdots;\\alpha_m)$임. $w,b$의 편도함수(partial derivative)에 대한 $L(w,b,\\alpha)$를 0으로 두면, 아래의 식을 얻을 수 있다.\n",
    "    - $$w=\\sum_{i=0}^m \\alpha_iy_ix_i,~~~~(2)\\\\ 0=\\sum_{i_1}^m \\alpha_iy_i~~~~~~~~~~(3)$$\n",
    "<br/><br/>\n",
    "- (2) 식을 (1)식에 대입하면 $L(w, b, \\alpha)$에 $w$와 $b$를 소거할 수 있고, 다시 (3) 식의 제약 조건을 고려하면 Dual problem을 얻음.\n",
    "$$\\max_{\\alpha} \\sum_{i=0}^m\\alpha_i - \\frac1 2\\sum_{i=0}^m\\sum_{j=0}^m\\alpha_i\\alpha_jy_iy_jx_i^Tx_j \\\\ s.t.~~\\sum_{i=0}^m\\alpha_iy_i=0,\\\\a_i\\geq0,~~~i=1,2,\\cdots,m.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693c7dee-fe0e-4abe-8aec-19091d3176c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
