{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94097142-c275-4fb3-a631-1b110ccb4fbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 과제\n",
    "### 2022235027 민현기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e812552c-574a-4a98-8652-facfc36cecfb",
   "metadata": {},
   "source": [
    "# 1. 첨부된 DASC705001_HW1_News-1.csv를 이용하여 아래 질문에 답하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c7e9cb-5b9b-4ca2-9b99-aa07a97daeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b360de-30a5-4f30-b797-07f5ac7d6d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('DASC705001_HW1_News-1.csv', encoding='latin1', low_memory=False)\n",
    "df = df.iloc[:, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf27433-529b-4546-8661-a9de3d7c1435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_nlp(pandas_text_columns):\n",
    "\n",
    "    clean = re.compile(\"[^A-Za-z ]\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # 특수문자, 숫자 제거\n",
    "    preprocessing_text = pandas_text_columns.apply(lambda x: clean.sub('', str(x)))\n",
    "    # 영소문자 변경\n",
    "    preprocessing_text = preprocessing_text.apply(lambda x: x.lower()).to_list()\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for words in tqdm.tqdm(preprocessing_text):\n",
    "        # 단어 토큰화\n",
    "        for word in words.split():\n",
    "            # 의미있는 단어를 추출하기 위하여 명사, 형용사 순으로 표제어 추출 진행\n",
    "            lemma = lemmatizer.lemmatize(word, pos='n')\n",
    "            lemma = lemmatizer.lemmatize(word, pos='a')            \n",
    "            # 불용어 및 단어길이 2이하 제거\n",
    "            if lemma not in stop_words and len(lemma) > 2:\n",
    "                result.append(lemma)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b237e6f6-69f6-4a5a-8958-3d5d2743aad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 많이 등장한 상위 25개 단어 선정\n",
    "def custom_most_common(words):\n",
    "    return [i[0] for i in FreqDist(words).most_common(25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bc87696-a9df-468a-9971-466609570abd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 31460/31460 [00:05<00:00, 5998.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 31460/31460 [01:40<00:00, 313.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 7087/7087 [00:00<00:00, 10952.29it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 7087/7087 [00:21<00:00, 331.95it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 24373/24373 [00:03<00:00, 7901.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 24373/24373 [01:19<00:00, 307.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# title\n",
    "title = custom_nlp(pandas_text_columns=df['title'])\n",
    "title = custom_most_common(title)\n",
    "\n",
    "# text\n",
    "text = custom_nlp(pandas_text_columns=df['text'])\n",
    "text = custom_most_common(text)\n",
    "\n",
    "# subject_worldnews_title\n",
    "worldnews_title = custom_nlp(pandas_text_columns=df[df['subject'] == 'worldnews']['title'])\n",
    "worldnews_title = custom_most_common(worldnews_title)\n",
    "\n",
    "# subject_worldnews_text\n",
    "worldnews_text = custom_nlp(pandas_text_columns=df[df['subject'] == 'worldnews']['text'])\n",
    "worldnews_text = custom_most_common(worldnews_text)\n",
    "\n",
    "# subject_US_news_title\n",
    "USnews_title = custom_nlp(pandas_text_columns=df[df['subject'] != 'worldnews']['title'])\n",
    "USnews_title = custom_most_common(USnews_title)\n",
    "\n",
    "# subject_US_news_text\n",
    "USnews_text = custom_nlp(pandas_text_columns=df[df['subject'] != 'worldnews']['text'])\n",
    "USnews_text = custom_most_common(USnews_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60574ace-cc56-43ef-873a-61a577c7a272",
   "metadata": {},
   "source": [
    "## 1-1. \"title\"과 \"text\"에서 가장 많이 등장하는 단어는 무엇인가? 본인이 제시할 수 있는 가장 유의미한 상위 10개의 단어를 각각 제시하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a025f80-266c-427d-8364-39eb416933df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title 상위 20개 단어: ['trump', 'video', 'says', 'obama', 'trumps', 'house', 'watch', 'hillary', 'new', 'white', 'president', 'clinton', 'bill', 'state', 'russia', 'donald', 'republican', 'north', 'court', 'black', 'election', 'news', 'media', 'senate', 'breaking']\n",
      "text 상위 20개 단어: ['said', 'trump', 'would', 'president', 'people', 'one', 'state', 'also', 'new', 'reuters', 'donald', 'house', 'states', 'government', 'clinton', 'republican', 'could', 'obama', 'united', 'told', 'white', 'like', 'campaign', 'last', 'two']\n",
      "\n",
      "----- title 선정 단어: ['trump', 'obama', 'house', 'hillary', 'white', 'president', 'clinton', 'bill', 'russia', 'republican']\n",
      "----- text 선정 단어: ['trump', 'president', 'people', 'house', 'government', 'clinton', 'republican', 'obama', 'united', 'white']\n"
     ]
    }
   ],
   "source": [
    "print(f'title 상위 20개 단어: {title}')\n",
    "print(f'text 상위 20개 단어: {text}\\n')\n",
    "\n",
    "# 'video', 'says', 'watch', 'reuters' 등 뉴스 그 자체와 연관된 단어 및 'state' 등 의미파악이 힘든 단어 위주로 제외 후 상위 10개 선정\n",
    "select_title = ['trump', 'obama', 'house', 'hillary', 'white', 'president', 'clinton', 'bill', 'russia', 'republican']\n",
    "select_text = ['trump', 'president', 'people', 'house', 'government', 'clinton', 'republican', 'obama', 'united', 'white']\n",
    "\n",
    "print(f'----- title 선정 단어: {select_title}')\n",
    "print(f'----- text 선정 단어: {select_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276e827a-d5e9-4da4-9242-d1f59aec6354",
   "metadata": {},
   "source": [
    "## 1-2. \"subject\"가 \"worldnews\"인 경우와 \"US_New\"의 경우 1-1의 답변은 어떻게 달라지는가? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f419e8dd-e2d1-4b05-808d-1e65c7ae08a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worldnews_title 상위 20개 단어: ['says', 'north', 'korea', 'trump', 'new', 'china', 'minister', 'talks', 'brexit', 'south', 'police', 'state', 'iran', 'russia', 'election', 'may', 'court', 'vote', 'president', 'syria', 'leader', 'deal', 'government', 'turkey', 'opposition']\n",
      "worldnews_text 상위 20개 단어: ['said', 'reuters', 'government', 'would', 'president', 'state', 'people', 'party', 'minister', 'also', 'told', 'united', 'last', 'two', 'north', 'one', 'new', 'military', 'year', 'could', 'states', 'country', 'trump', 'security', 'korea']\n",
      "\n",
      "----- worldnews_title 선정 단어: ['north', 'korea', 'trump', 'china', 'minister', 'brexit', 'south', 'police', 'iran', 'russia']\n",
      "----- worldnews_text 선정 단어: ['government', 'president', 'people', 'party', 'minister', 'united', 'north', 'military', 'country']\n"
     ]
    }
   ],
   "source": [
    "print(f'worldnews_title 상위 20개 단어: {worldnews_title}')\n",
    "print(f'worldnews_text 상위 20개 단어: {worldnews_text}\\n')\n",
    "\n",
    "select_worldnews_title = ['north', 'korea', 'trump', 'china', 'minister', 'brexit', 'south', 'police', 'iran', 'russia']\n",
    "select_worldnews_text = ['government', 'president', 'people', 'party', 'minister', 'united', 'north', 'military', 'country']\n",
    "\n",
    "print(f'----- worldnews_title 선정 단어: {select_worldnews_title}')\n",
    "print(f'----- worldnews_text 선정 단어: {select_worldnews_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f72d2165-e3c0-48c8-905f-1cb99e771508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USnews_title 상위 20개 단어: ['trump', 'video', 'obama', 'says', 'trumps', 'house', 'watch', 'hillary', 'white', 'clinton', 'new', 'president', 'bill', 'donald', 'republican', 'black', 'breaking', 'news', 'senate', 'republicans', 'media', 'russia', 'gop', 'campaign', 'state']\n",
      "USnews_text 상위 20개 단어: ['trump', 'said', 'would', 'president', 'people', 'one', 'donald', 'house', 'new', 'clinton', 'republican', 'also', 'obama', 'state', 'states', 'white', 'like', 'campaign', 'could', 'news', 'united', 'time', 'told', 'hillary', 'even']\n",
      "\n",
      "----- USnews_title 선정 단어: ['trump', 'obama', 'house', 'hillary', 'white', 'clinton', 'president', 'bill', 'republican', 'black']\n",
      "----- USnews_text 선정 단어: ['trump', 'president', 'people', 'house', 'clinton', 'republican', 'obama', 'white', 'campaign', 'united']\n"
     ]
    }
   ],
   "source": [
    "print(f'USnews_title 상위 20개 단어: {USnews_title}')\n",
    "print(f'USnews_text 상위 20개 단어: {USnews_text}\\n')\n",
    "\n",
    "select_USnews_title = ['trump', 'obama', 'house', 'hillary', 'white', 'clinton', 'president', 'bill', 'republican', 'black']\n",
    "select_USnews_text = ['trump', 'president', 'people', 'house', 'clinton', 'republican', 'obama', 'white', 'campaign', 'united']\n",
    "\n",
    "print(f'----- USnews_title 선정 단어: {select_USnews_title}')\n",
    "print(f'----- USnews_text 선정 단어: {select_USnews_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e270d039-2e1b-4d87-9989-96a909874802",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1-3. 위 1-1과 1-2에서 찾은 60개(중복 포함)의 단어의 TF-IDF는 얼마인가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdc7c54e-2607-48f4-863f-033322d36c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [(' ').join(i) for i in [select_title, select_text,select_worldnews_title, select_worldnews_text, select_USnews_title, select_USnews_text]]\n",
    "tfidfv = TfidfVectorizer().fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5becb2f-efa8-4bff-b1fd-0a16e8cdc5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf 결과: [[0.39535363 0.         0.         0.         0.         0.28602813\n",
      "  0.         0.         0.39535363 0.28602813 0.         0.\n",
      "  0.         0.         0.         0.28602813 0.         0.\n",
      "  0.         0.24700817 0.28602813 0.39535363 0.         0.24700817\n",
      "  0.         0.28602813]\n",
      " [0.         0.         0.         0.         0.         0.29980346\n",
      "  0.         0.41439416 0.         0.29980346 0.         0.\n",
      "  0.         0.         0.         0.29980346 0.         0.34986002\n",
      "  0.         0.25890427 0.29980346 0.         0.         0.25890427\n",
      "  0.34986002 0.29980346]\n",
      " [0.         0.         0.34752929 0.         0.34752929 0.\n",
      "  0.         0.         0.         0.         0.34752929 0.34752929\n",
      "  0.         0.28497899 0.28497899 0.         0.         0.\n",
      "  0.34752929 0.         0.         0.28497899 0.34752929 0.17804854\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.40037358 0.32831206 0.         0.         0.         0.\n",
      "  0.40037358 0.32831206 0.32831206 0.         0.40037358 0.2771836\n",
      "  0.         0.20512208 0.         0.         0.         0.\n",
      "  0.2771836  0.        ]\n",
      " [0.38110999 0.46476017 0.         0.         0.         0.27572322\n",
      "  0.         0.         0.38110999 0.27572322 0.         0.\n",
      "  0.         0.         0.         0.27572322 0.         0.\n",
      "  0.         0.23810906 0.27572322 0.         0.         0.23810906\n",
      "  0.         0.27572322]\n",
      " [0.         0.         0.         0.4854522  0.         0.28799896\n",
      "  0.         0.         0.         0.28799896 0.         0.\n",
      "  0.         0.         0.         0.28799896 0.         0.33608459\n",
      "  0.         0.24871014 0.28799896 0.         0.         0.24871014\n",
      "  0.33608459 0.28799896]]\n",
      "\n",
      "tf-idf 벡터 사이즈: (6, 26)\n",
      "\n",
      "{'trump': 23, 'obama': 15, 'house': 9, 'hillary': 8, 'white': 25, 'president': 19, 'clinton': 5, 'bill': 0, 'russia': 21, 'republican': 20, 'people': 17, 'government': 7, 'united': 24, 'north': 14, 'korea': 11, 'china': 4, 'minister': 13, 'brexit': 2, 'south': 22, 'police': 18, 'iran': 10, 'party': 16, 'military': 12, 'country': 6, 'black': 1, 'campaign': 3}\n"
     ]
    }
   ],
   "source": [
    "print(f'tf-idf 결과: {tfidfv.transform(corpus).toarray()}\\n')\n",
    "print(f'tf-idf 벡터 사이즈: {tfidfv.transform(corpus).toarray().shape}\\n')\n",
    "print(tfidfv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46377792-be89-439b-af6f-c40c8c880b46",
   "metadata": {},
   "source": [
    "## 2. 본인이 1 분석을 수행하기 위해 이용한 Library를 명시하고 해당 Library에서 사용하고 있는 TF-IDF는 어떤 공식을 따르는지 제시하라.\n",
    "\n",
    "- 단어 토큰화, 표제어 추출 및 불용어 제거 등 기본 전처리는 nltk에서 수행 \n",
    "- TF-IDF는 scikit learn에서 제공하는 함수를 사용함\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*cYuqqICc7nyNGBEn4Fg4Ag.png)\n",
    "\n",
    "\n",
    "- $Tf(t,d)$: 특정한 단어의 빈도 수\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*tOMCF5_plhul9yAS8eSl9g.png)\n",
    "    \n",
    "- $idf(t)$: 특정한 단어가 들어 있는 문서의 수에 반비례하는 수\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*dAv0xsTO24_ywJOuJM_ObA.png)\n",
    "    \n",
    "    \n",
    "- Sklearn에서는 Idf를 계산하는 방식이 표준 방법과는 다름\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*8TT1FmB6Kvl5PoDvozbWtQ.png)\n",
    "\n",
    "    - Sklearn에서는 0으로 나누는 것을 피하기 위하여 분자와 분모 모두에 1을 더하고, 추가적으로 상수 1을 더함\n",
    "    - 그리고, log의 밑수로 자연상수를 활용하는 자연로그를 활용하며, 표준화된 방법에서는 log의 밑으로 10을 활용\n",
    "    - 또한, 원시 tf-idf 벡터가 주어지면 L2 정규화를 수행하여 값을 조정하여 TF-idf를 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebc502a-9096-44cf-b72c-b810262ce55e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
